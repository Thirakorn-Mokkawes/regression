{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What are some common assumptions made in linear regression, and why are they important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression relies on several key assumptions to ensure the validity and reliability of the model's predictions. These assumptions are important because violations can lead to biased or misleading results. Here are the common assumptions made in linear regression:\n",
    "\n",
    "## 1.1. Linearity\n",
    "\n",
    "**Assumption:**  \n",
    "- The relationship between the independent variables and the dependent variable is linear.\n",
    "\n",
    "**Importance:**  \n",
    "- If the true relationship is not linear, the model's predictions may be inaccurate. Non-linear relationships might require transformation of variables or using non-linear models.\n",
    "\n",
    "## 1.2. Independence\n",
    "\n",
    "**Assumption:**  \n",
    "- The observations are independent of each other.\n",
    "\n",
    "**Importance:**  \n",
    "- This assumption ensures that the model's errors are not correlated. Violations (e.g., time series data) might require techniques like time series analysis or using methods that account for correlations.\n",
    "\n",
    "## 1.3. Homoscedasticity\n",
    "\n",
    "**Assumption:**  \n",
    "- The variance of the error terms is constant across all levels of the independent variables.\n",
    "\n",
    "**Importance:**  \n",
    "- If the error variance changes (heteroscedasticity), it can affect the efficiency and reliability of the coefficient estimates. It can lead to biased standard errors, affecting hypothesis tests.\n",
    "\n",
    "## 1.4. Normality of Errors\n",
    "\n",
    "**Assumption:**  \n",
    "- The error terms are normally distributed.\n",
    "\n",
    "**Importance:**  \n",
    "- Normality is important for inference, particularly for calculating confidence intervals and performing hypothesis tests. In large samples, the Central Limit Theorem often mitigates this issue, but non-normality in small samples can be problematic.\n",
    "\n",
    "## 1.5. No Multicollinearity\n",
    "\n",
    "**Assumption:**  \n",
    "- The independent variables are not highly correlated with each other.\n",
    "\n",
    "**Importance:**  \n",
    "- Multicollinearity can lead to instability in the coefficient estimates, making it difficult to determine the effect of each independent variable. It can also inflate the variance of the estimates, leading to less reliable results.\n",
    "\n",
    "## 1.6. No Autocorrelation\n",
    "\n",
    "**Assumption:**  \n",
    "- The residuals (errors) are not correlated with each other.\n",
    "\n",
    "**Importance:**  \n",
    "- Autocorrelation, particularly in time series data, can indicate that the model is missing key patterns. This violation suggests that the model needs additional lagged variables or other adjustments.\n",
    "\n",
    "## Why Assumptions are Important\n",
    "\n",
    "- **Model Validity:** Ensuring assumptions hold means the model's results are trustworthy and valid.\n",
    "- **Interpretability:** Linear regression's interpretability relies on these assumptions being met, providing clear insights into variable relationships.\n",
    "- **Inference Accuracy:** Accurate hypothesis testing, confidence intervals, and predictions depend on the assumptions being satisfied.\n",
    "\n",
    "## Detecting and Addressing Violations\n",
    "\n",
    "1. **Linearity:** Use scatter plots or residual plots to check for non-linear patterns.\n",
    "2. **Independence:** For time series data, consider using time series models.\n",
    "3. **Homoscedasticity:** Plot residuals versus fitted values to check for constant variance. Transformations or weighted least squares can help address violations.\n",
    "4. **Normality:** Use Q-Q plots or statistical tests (e.g., Shapiro-Wilk) to check for normality. Consider transformations if needed.\n",
    "5. **Multicollinearity:** Calculate Variance Inflation Factor (VIF) for each independent variable. Consider removing or combining correlated variables.\n",
    "6. **Autocorrelation:** Use plots like the Durbin-Watson statistic to detect autocorrelation. Adjust the model with lag variables if necessary.\n",
    "\n",
    "By ensuring these assumptions are met or appropriately addressing violations, linear regression models can provide reliable and meaningful insights. Let me know if you need more details on any of these assumptions or their implications!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
