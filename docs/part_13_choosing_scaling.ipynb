{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Choosing the Right Type of Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Standardization (Z-score Normalization)\n",
    "\n",
    "- When to Use:\n",
    "\n",
    "    - When you want the features to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "    - When the model assumes normally distributed data.\n",
    "\n",
    "    - When features have different units or scales.\n",
    "\n",
    "- Advantages:\n",
    "\n",
    "    - Useful for algorithms that assume data is normally distributed (e.g., linear regression, logistic regression, SVM).\n",
    "\n",
    "    - Handles features with different units.\n",
    "\n",
    "- Limitations:\n",
    "\n",
    "    - Sensitive to outliers, which can skew the mean and standard deviation.\n",
    "\n",
    "## 1.2. Min-Max Scaling\n",
    "\n",
    "- When to Use:\n",
    "\n",
    "    - When you want to scale features to a specific range, typically [0, 1].\n",
    "\n",
    "    - Useful for algorithms sensitive to the magnitude of data (e.g., neural networks).\n",
    "\n",
    "- Advantages:\n",
    "\n",
    "    - Maintains the relationships between data points by scaling them proportionally.\n",
    "\n",
    "    - Useful for preserving zero sparsity.\n",
    "\n",
    "- Limitations:\n",
    "\n",
    "    - Sensitive to outliers, which can distort the range.\n",
    "\n",
    "## 1.3. Robust Scaling\n",
    "\n",
    "- When to Use:\n",
    "\n",
    "    - When the data contains significant outliers or is not normally distributed.\n",
    "\n",
    "- Advantages:\n",
    "\n",
    "    - Uses the median and interquartile range, making it robust to outliers.\n",
    "\n",
    "    - Suitable for datasets with skewed distributions.\n",
    "\n",
    "- Limitations:\n",
    "\n",
    "    - Less effective when the dataset has a small number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Situations Where Scaling Might Not Be Necessary\n",
    "\n",
    "- **Homogeneous Features**\n",
    "\n",
    "    - If all features are on the same scale and have similar units, scaling might not significantly impact the model's performance.\n",
    "\n",
    "- **Tree-Based Models**\n",
    "\n",
    "    - Algorithms like decision trees, random forests, and gradient boosting do not require feature scaling because they are based on conditions and splits rather than distances or magnitudes.\n",
    "\n",
    "- **Linear Models without Regularization**\n",
    "\n",
    "    - In simple linear regression (without regularization), scaling is not strictly necessary, but it can improve interpretability and convergence speed in the optimization process.\n",
    "\n",
    "- **When Interpretability is Key**\n",
    "\n",
    "    - If interpretability is more important than model performance, keeping the original scale of features might be preferable for better understanding and communication of results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "margaret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
