{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multicollinearity in multiple linear regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity is a statistical phenomenon that occurs when two or more independent variables in a multiple regression model are highly correlated. This means that one independent variable can be linearly predicted from the others with a substantial degree of accuracy. Multicollinearity can cause problems in estimating the coefficients of the regression model, making it difficult to determine the effect of each independent variable on the dependent variable. \n",
    "\n",
    "## 1.1. Disadvantages of multicollinearity\n",
    "\n",
    "The disadvantages of multicollinearity in a regression model primarily relate to interpretation and estimation accuracy. Here are some of the main issues caused by multicollinearity:\n",
    "\n",
    "1. **Unstable Coefficient Estimates**\n",
    "\n",
    "    - When multicollinearity is present, small changes in the data or model specification can lead to large changes in the estimated coefficients. This instability makes it difficult to rely on the coefficients for understanding relationships between predictors and the response variable.\n",
    "\n",
    "2. **Increased Standard Errors**\n",
    "\n",
    "    - Multicollinearity inflates the standard errors of the coefficients, making them less precise. This inflation reduces the statistical power of hypothesis tests, making it harder to detect significant relationships.\n",
    "\n",
    "3. **Difficulty in Assessing the Importance of Predictors**\n",
    "\n",
    "    - When predictors are highly correlated, it becomes challenging to determine the individual contribution of each predictor to the model. Coefficients may be counterintuitive or misleading, as changes in one predictor are often associated with changes in others.\n",
    "\n",
    "4. **Misleading Significance Tests**\n",
    "\n",
    "    - High multicollinearity can cause coefficients to appear statistically insignificant even when they should be significant. This can lead to incorrect conclusions about which predictors are important.\n",
    "\n",
    "5. **Overfitting and Model Interpretation**\n",
    "\n",
    "    - Multicollinearity can contribute to overfitting, where the model fits the training data very well but performs poorly on new data. Additionally, interpreting a model with multicollinearity is problematic since itâ€™s unclear which variables are driving the results.\n",
    "\n",
    "6. **Limited Extrapolation**\n",
    "\n",
    "    - Models with multicollinearity may not generalize well to other datasets. Predictions made outside the range of the data can be particularly unreliable.\n",
    "\n",
    "7. **Complicated Variable Selection**\n",
    "\n",
    "    - In the presence of multicollinearity, standard variable selection techniques (e.g., stepwise selection) may not work effectively, as removing or adding predictors can dramatically change the model coefficients.\n",
    "\n",
    "8. **Increased Sensitivity to Multivariate Outliers**\n",
    "\n",
    "    - Models with multicollinearity are more sensitive to outliers in the predictor variables, which can disproportionately affect the model's stability and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Techniques to handle multicollinearity\n",
    "\n",
    "### 1.2.1. Detecting Multicollinearity\n",
    "\n",
    "Before addressing multicollinearity, it is important to detect it using the following techniques:\n",
    "\n",
    "- **Correlation Matrix**\n",
    "\n",
    "    - Calculate the correlation matrix to identify pairs of variables with high correlation coefficients (typically above 0.8 or 0.9).\n",
    "\n",
    "    - High correlation between two variables indicates that they might be explaining the same variance in the dependent variable.\n",
    "\n",
    "- **Variance Inflation Factor (VIF)**\n",
    "\n",
    "    - Calculate the VIF for each independent variable. A VIF value greater than 5 or 10 indicates a multicollinearity problem.\n",
    "\n",
    "    - The formula for VIF is:\n",
    "\n",
    "$$\n",
    "\\text{VIF}(X_i) = \\frac{1}{1 - R_i^2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $R_i^2$ is the R-squared value obtained by regressing the variable $X_i$ on all other independent variables.\n",
    "- A higher VIF indicates a higher level of multicollinearity.\n",
    "\n",
    "By assessing the VIF values and the correlation matrix, you can identify and address potential multicollinearity issues in your regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Methods to Handle Multicollinearity\n",
    "\n",
    "- **Remove Highly Correlated Predictors**\n",
    "\n",
    "  - **Identify and Remove:** If two or more variables are highly correlated, consider removing one of them, especially if it does not add significant value to the model. Use domain knowledge to decide which variable to retain.\n",
    "\n",
    "- **Combine Variables**\n",
    "\n",
    "  - **Create Composite Variables:** Combine correlated variables into a single composite variable, such as by taking the average or sum. This can reduce multicollinearity while retaining the explanatory power.\n",
    "\n",
    "- **Principal Component Analysis (PCA)**\n",
    "\n",
    "  - **Dimensionality Reduction:** Use PCA to transform correlated variables into a smaller set of uncorrelated variables (principal components). These components can then be used as predictors in the regression model.\n",
    "\n",
    "- **Regularization Techniques**\n",
    "\n",
    "  - **Ridge Regression (L2 Regularization):** Add a penalty term to the loss function proportional to the square of the coefficients, which helps to shrink the coefficients of correlated variables and reduce multicollinearity.\n",
    "\n",
    "  - The loss function for Ridge Regression is:\n",
    "\n",
    "  $$\n",
    "  \\text{Loss} = \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum w_i^2\n",
    "  $$\n",
    "\n",
    "  - **Lasso Regression (L1 Regularization):** Add a penalty term proportional to the absolute value of the coefficients, which can shrink some coefficients to zero and effectively perform variable selection.\n",
    "\n",
    "  - The loss function for Lasso Regression is:\n",
    "\n",
    "  $$\n",
    "  \\text{Loss} = \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum |w_i|\n",
    "  $$\n",
    "\n",
    "- **Partial Least Squares Regression (PLS)**\n",
    "  - **PLS Regression:** Similar to PCA, PLS reduces the predictors to a smaller set of uncorrelated components, but it also considers the response variable, ensuring the components are relevant for prediction.\n",
    "\n",
    "- **Feature Selection Techniques**\n",
    "  - **Backward Elimination, Forward Selection, or Stepwise Selection:** Use these techniques to iteratively add or remove variables based on their statistical significance and contribution to the model, which can help reduce multicollinearity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "margaret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
