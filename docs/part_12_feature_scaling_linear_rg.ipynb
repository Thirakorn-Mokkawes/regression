{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is an important preprocessing step in linear regression and other machine learning algorithms. It ensures that all features contribute equally to the model's performance and that the optimization process works efficiently. Feature scaling is particularly important when using regularization techniques like Ridge and Lasso regression, which are sensitive to the scale of input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Why Feature Scaling Matters\n",
    "\n",
    "1. **Effect on Coefficient Estimates**\n",
    "\n",
    "    - In linear regression, the scale of the input features affects the magnitude of the estimated coefficients. Features with larger scales can dominate those with smaller scales, leading to biased interpretations of the model.\n",
    "\n",
    "2. **Optimization Efficiency**\n",
    "\n",
    "    - Linear regression uses gradient-based optimization techniques. If the features have different scales, the optimization process may converge slowly or become unstable. Scaling helps achieve a smoother gradient descent path.\n",
    "\n",
    "3. **Comparability of Features**\n",
    "\n",
    "    - Scaling allows for fair comparison between coefficients in the model, making it easier to interpret their relative importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Importance of Scaling in Regularization\n",
    "\n",
    "Regularization techniques like Ridge and Lasso introduce penalties based on the magnitude of coefficients. The scale of the features can significantly impact the effectiveness of these penalties:\n",
    "\n",
    "1. **Ridge Regression (L2 Regularization)**\n",
    "\n",
    "    - Ridge adds a penalty equal to the square of the coefficients. If features are on different scales, the penalty term can disproportionately affect features with larger scales, leading to suboptimal coefficient estimates.\n",
    "\n",
    "2. **Lasso Regression (L1 Regularization)**\n",
    "\n",
    "    - Lasso adds a penalty proportional to the absolute value of the coefficients. Like Ridge, Lasso can unfairly penalize features based on their scale, potentially setting important features with small scales to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Types of Feature Scaling\n",
    "\n",
    "1. **Standardization (Z-score Normalization)**\n",
    "\n",
    "  - Transforms the data to have a mean of 0 and a standard deviation of 1.\n",
    "  - **Formula:** \n",
    "\n",
    "  $$\n",
    "  z = \\frac{x - \\mu}{\\sigma}\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $x$ is the feature value.\n",
    "  - $\\mu$ is the mean of the feature.\n",
    "  - $\\sigma$ is the standard deviation.\n",
    "\n",
    "2. **Min-Max Scaling**\n",
    "\n",
    "  - Transforms the data to a fixed range, usually [0, 1].\n",
    "  - **Formula:** \n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $x'$ is the scaled feature value.\n",
    "\n",
    "3. **Robust Scaling**\n",
    "\n",
    "  - Uses the median and the interquartile range for scaling, making it robust to outliers.\n",
    "  - **Formula:** \n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - \\text{median}(x)}{\\text{IQR}(x)}\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $\\text{IQR}$ is the interquartile range.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
