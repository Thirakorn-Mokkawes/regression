{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Metrics of regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Evaluating the performance of regression models is essential for understanding their accuracy, reliability, and suitability for making predictions. Several metrics are commonly used to assess regression models, each providing different insights into the model's performance. Here's an overview of these metrics and how they help evaluate regression models:\n",
    "\n",
    "## 1.1. Mean Absolute Error (MAE)\n",
    "\n",
    "- **Definition**\n",
    "\n",
    "    - MAE measures the average magnitude of the errors between predicted and actual values, ignoring their direction. It is the mean of the absolute differences between predicted and actual values.\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "    - $n$ is the number of observations.\n",
    "    - $y_i$ is the actual value.\n",
    "    - $\\hat{y}_i$ is the predicted value.\n",
    "\n",
    "- **Interpretation**\n",
    "\n",
    "    - MAE provides a straightforward measure of average error magnitude. It is useful for understanding how far off predictions are, on average, from the actual values.\n",
    "\n",
    "- **Advantages**\n",
    "\n",
    "    - Easy to interpret and understand.\n",
    "\n",
    "    - Not sensitive to outliers as it does not square errors.\n",
    "\n",
    "- **Disadvantages:**\n",
    "\n",
    "    - Does not provide information about the direction of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Mean Squared Error (MSE)\n",
    "\n",
    "- **Definition**\n",
    "\n",
    "    - MSE measures the average of the squares of the errors. It penalizes larger errors more significantly than smaller ones.\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "    - $n$ is the number of observations.\n",
    "    - $y_i$ is the actual value.\n",
    "    - $\\hat{y}_i$ is the predicted value.\n",
    "\n",
    "- **Interpretation**\n",
    "\n",
    "    - MSE gives more weight to larger errors, making it useful for identifying models with large prediction errors. It helps assess the overall accuracy of the model.\n",
    "\n",
    "- **Advantages**\n",
    "\n",
    "    - Highlights larger errors due to squaring, providing a more sensitive measure.\n",
    "\n",
    "- **Disadvantages**\n",
    "\n",
    "    - Sensitive to outliers, which can disproportionately affect the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.  Root Mean Squared Error (RMSE)\n",
    "\n",
    "- **Definition**\n",
    "\n",
    "    - RMSE is the square root of the MSE. It brings the error metric to the same scale as the target variable, making it easier to interpret.\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "    - $n$ is the number of observations.\n",
    "    - $y_i$ is the actual value.\n",
    "    - $\\hat{y}_i$ is the predicted value.\n",
    "\n",
    "- **Interpretation**\n",
    "\n",
    "    - RMSE provides a measure of error magnitude that is interpretable on the same scale as the original data, helping to assess how much prediction errors deviate from actual values on average.\n",
    "\n",
    "- **Advantages**\n",
    "\n",
    "    - Easy to interpret and compare with the scale of the actual values.\n",
    "\n",
    "- **Disadvantages**\n",
    "\n",
    "    - Like MSE, it is sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. R-squared ($R^2$)\n",
    "\n",
    "- **Definition**\n",
    "\n",
    "    - $R^2$ , or the coefficient of determination, measures the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "    - $n$ is the number of observations.\n",
    "    - $y_i$ is the actual value.\n",
    "    - $\\hat{y}_i$ is the predicted value.\n",
    "    - $\\bar{y}$ is the mean of the actual values.\n",
    "\n",
    "- **Interpretation**\n",
    "\n",
    "    - $R^2$  ranges from 0 to 1, where 1 indicates that the model explains all the variance in the data, and 0 indicates none. Higher $R^2$ values generally suggest a better fit.\n",
    "\n",
    "- **Advantages**\n",
    "\n",
    "    - Provides an intuitive measure of how well the model fits the data.\n",
    "\n",
    "- **Disadvantages**\n",
    "\n",
    "    - Can be misleading in models with many variables, as it tends to increase with additional predictors, regardless of their significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Adjusted R-squared\n",
    "\n",
    "- **Definition**\n",
    "\n",
    "    - Adjusted $R^2$ is a modified version of $R^2$ that adjusts for the number of predictors in the model, preventing the overestimation of the model fit by accounting for the number of predictors.\n",
    "\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{1-R^2}{n-p-1} \\right) \\times (n-1)\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "    - $R^2$ is the R-squared value.\n",
    "    - $n$ is the number of observations.\n",
    "    - $p$ is the number of predictors (independent variables) in the model.\n",
    "\n",
    "- **Interpretation**\n",
    "\n",
    "    - Adjusted $R^2$ provides a more accurate measure of the goodness-of-fit when comparing models with a different number of predictors.\n",
    "\n",
    "- **Advantages**\n",
    "\n",
    "    - Penalizes the addition of irrelevant predictors, discouraging overfitting.\n",
    "\n",
    "- **Disadvantages**\n",
    "\n",
    "    - Can still be affected by multicollinearity and does not indicate if predictors are meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "- **Definition**\n",
    "\n",
    "    - MAPE measures the accuracy as a percentage by calculating the average absolute percentage error between predicted and actual values.\n",
    "\n",
    "$$\n",
    "\\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "    - $n$ is the number of observations.\n",
    "    - $y_i$ is the actual value.\n",
    "    - $\\hat{y}_i$ is the predicted value.\n",
    "\n",
    "- **Interpretation**\n",
    "\n",
    "    - MAPE expresses error as a percentage of the actual values, making it easy to understand and compare across different datasets.\n",
    "\n",
    "- **Advantages**\n",
    "\n",
    "    - Scaled to percentages, making it easily interpretable.\n",
    "\n",
    "- **Disadvantages**\n",
    "\n",
    "    - Can be misleading with very small actual values, leading to inflated percentage errors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
