{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Could you explain what linear regression is and how it works in simple terms?\n",
    "\n",
    "Linear regression is one of the simplest and most widely used statistical techniques in machine learning for predicting a continuous outcome. It attempts to model the relationship between two or more variables by fitting a linear equation to observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Key Concepts of Linear Regression\n",
    "### 1.1.1. Dependent and Independent Variables\n",
    "- **Dependent Variable (Target):** The variable we are trying to predict, often denoted as $y$.\n",
    "- **Independent Variable(s) (Features):** The variable(s) used to predict the target, often denoted as $x$. In cases with more than one independent variable, they can be denoted as $x_1,x_2,\\ldots,x_n$.\n",
    "### 1.1.2. Linear Equation\n",
    "- The relationship between the independent and dependent variables is expressed as a linear equation:\n",
    "$$\n",
    "y = mx + c\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the dependent variable.\n",
    "- \\( m \\) is the slope of the line.\n",
    "- \\( x \\) is the independent variable.\n",
    "- \\( c \\) is the y-intercept.\n",
    "\n",
    "or in multiple variables:\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( $y$ \\) is the dependent variable.\n",
    "- \\( $w_1, w_2, \\ldots, w_n$\\) are the weights (or coefficients) for each feature.\n",
    "- \\( $x_1, x_2, \\ldots, x_n$ \\) are the features (or independent variables).\n",
    "- \\( $b$ \\) is the bias term (or intercept).\n",
    "### 1.1.3. Objective\n",
    "- The objective of linear regression is to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the difference between the predicted values and the actual values in the dataset.\n",
    "\n",
    "### 1.1.4. Loss Function\n",
    "- The most common loss function used in linear regression is the Mean Squared Error (MSE), which measures the average of the squares of the errors (the differences between the predicted and actual values).\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( $N$ \\) is the number of observations.\n",
    "- \\( $y_i$ \\) is the actual value.\n",
    "- \\( $\\hat{y}_i$ \\) is the predicted value.\n",
    "\n",
    "### 1.1.5. Optimization\n",
    "- The goal is to find the coefficients ($m$ and $c$ or $w$ and $b$) that minimize the MSE. This is typically done using techniques like **Gradient Descent** or the **Normal Equation**.\n",
    "\n",
    "## 1.2. How Linear Regression Works\n",
    "1. **Data Collection**\n",
    "    - Gather a dataset with known values for both the independent variable(s) and the dependent variable.\n",
    "2. **Model Training**\n",
    "    - Use the dataset to estimate the coefficients ($m$ and $c$ in simple linear regression, or $w$ and $b$ in multiple linear regression) by minimizing the MSE.\n",
    "3. **Prediction**\n",
    "    - Once the model is trained, use the linear equation to predict new values of $y$ for given values of $x$.\n",
    "4. **Evaluation**\n",
    "    - Evaluate the model's performance using metrics like MSE, RMSE, or $R^2$ (coefficient of determination), which measures how well the predicted values match the actual data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
