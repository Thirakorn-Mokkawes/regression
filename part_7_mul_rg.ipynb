{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multiple linear regression compared to simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression can easily extend to handle multiple independent variables through a technique known as multiple linear regression. This approach allows the model to account for the combined effect of several predictors on the dependent variable, providing a more comprehensive analysis than simple linear regression, which involves only one predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple linear regression, the relationship between the dependent variable ($y$) and multiple independent variables ($x_1, x_2, \\ldots, x_n$) is expressed as:\n",
    "\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the dependent variable.\n",
    "\n",
    "- $x_1, x_2, \\ldots, x_n$ are the independent variables.\n",
    "\n",
    "- $w_1, w_2, \\ldots, w_n$ are the coefficients (weights) of the independent variables, indicating the contribution of each variable to the prediction.\n",
    "\n",
    "- $b$ is the intercept (bias term), representing the expected value of $y$ when all independent variables are zero.\n",
    "\n",
    "**How It Works**\n",
    "1. **Data Collection:** Gather data for the dependent variable and all independent variables.\n",
    "\n",
    "2. **Model Fitting:** Use the least squares method to estimate the coefficients ($w_1,w_2, \\ldots, b$) that minimize the sum of the squared differences between observed and predicted values.\n",
    "\n",
    "3. **Prediction:** Use the fitted model to predict $y$ values for new observations.\n",
    "\n",
    "4. **Evaluation:** Assess model performance using metrics like R-squared, adjusted R-squared, Mean Squared Error (MSE), etc.\n",
    "\n",
    "## 1.1. Implications of Using Multiple Linear Regression\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "1. **Better Representation:** Multiple linear regression can capture more complex relationships by considering the combined influence of multiple variables, leading to more accurate predictions.\n",
    "\n",
    "2. **Control for Confounding Variables:** Including additional relevant variables helps isolate the effect of each predictor, controlling for potential confounding factors.\n",
    "\n",
    "3. **Improved Explanatory Power:** By accounting for more variables, the model can explain a larger portion of the variance in the dependent variable, as reflected in a higher R-squared value.\n",
    "\n",
    "**Challenges**\n",
    "\n",
    "1. **Multicollinearity:** When independent variables are highly correlated, it can lead to unstable coefficient estimates and difficulty in interpreting the model. This can be addressed by removing correlated variables, using regularization, or dimensionality reduction techniques like PCA.\n",
    "\n",
    "2. **Overfitting:** Including too many predictors increases the risk of overfitting, where the model captures noise rather than the underlying relationship. This can be mitigated through techniques like cross-validation and regularization (e.g., Lasso, Ridge regression).\n",
    "\n",
    "3. **Complexity and Interpretability:** As the number of predictors increases, the model becomes more complex and harder to interpret, especially if interactions between variables are involved.\n",
    "\n",
    "4. **Data Requirements:** More predictors require larger datasets to ensure reliable coefficient estimates and prevent overfitting.\n",
    "\n",
    "## 1.2. Comparison with Simple Linear Regression\n",
    "\n",
    "1. **Complexity:**\n",
    "    \n",
    "    - **Simple Linear Regression:** Involves one predictor, making it straightforward to interpret and visualize.\n",
    "    \n",
    "    - **Multiple Linear Regression:** Involves multiple predictors, providing a more comprehensive analysis but at the cost of increased complexity.\n",
    "\n",
    "2. **Explanatory Power:**\n",
    "\n",
    "    - **Simple Linear Regression:** Limited to explaining variance with one variable, which might not capture the full picture.\n",
    "\n",
    "    - **Multiple Linear Regression:** Captures the combined effects of several variables, often resulting in better predictive performance.\n",
    "\n",
    "3. **Use Cases:**\n",
    "\n",
    "    - **Simple Linear Regression:** Suitable for cases where a single variable is believed to strongly influence the outcome.\n",
    "\n",
    "    - **Multiple Linear Regression:** Ideal for scenarios with multiple factors contributing to the outcome.\n",
    "\n",
    "## 1.3. Example Implementation in Python\n",
    "Here's a simple example of implementing multiple linear regression using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [4.86875977 1.76007822 4.08711696]\n",
      "Intercept: 3.016337012470937\n",
      "Mean Squared Error: 1.10\n",
      "R-squared: 0.77\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Sample dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 3)  # Three independent variables\n",
    "y = 3 + 5*X[:, 0] + 2*X[:, 1] + 4*X[:, 2] + np.random.randn(100)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Coefficients: {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "margaret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
